{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we don't use the DATA PREPROCESSING step bcoz\n",
    "we don't have any table or any seperate columns for dependent and independent variables\n",
    "We have images to classify which are represented in terms of pixels\n",
    "\n",
    "So, what we do is we create seperate folders in our dataset\n",
    "-> Training Set\n",
    "-> Test Set\n",
    "\n",
    "And within it we have seperate sets for dogs and cats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 -> Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amit_bahir/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Step 1 -> Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3),activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 -> Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Dvides the size by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding another Convolution layer\n",
    "#     We need not use the input_shape as the input is not the image but it is the result\n",
    "#     obtained from the previous two layers...\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 -> Flattening\n",
    "classifier.add(Flatten())\n",
    "# Contains all the info about the spatial structure of our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 -> Full Connection\n",
    "#     Adding the hidden layer\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "#     Adding the output layer\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, we do IMAGE AUGMENTATION\n",
    "It's a technique that allows us to enrich our data sets(training set) without adding more images\n",
    "and therefore that allows us to get good performance results with little or no overfittingeven with a small \n",
    "amount of images.\n",
    "\n",
    "When we have few data in our training set then our modek finds correlations in those few obs of the training set\n",
    "but fails to generalize these correlations on some new obs.\n",
    "\n",
    "Image augmentation applies some random transformations as rotation, flipping, shifting or even shearing them\n",
    "and we get a lot of diverse batches and a lot more material to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/amit_bahir/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 1744s 218ms/step - loss: 0.4720 - acc: 0.7688 - val_loss: 0.5475 - val_acc: 0.7726\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 1796s 225ms/step - loss: 0.2923 - acc: 0.8738 - val_loss: 0.7597 - val_acc: 0.7571\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 2464s 308ms/step - loss: 0.1857 - acc: 0.9242 - val_loss: 1.0170 - val_acc: 0.7468\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 2516s 315ms/step - loss: 0.1302 - acc: 0.9490 - val_loss: 1.3601 - val_acc: 0.7330\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 2592s 324ms/step - loss: 0.0975 - acc: 0.9635 - val_loss: 1.3804 - val_acc: 0.7573\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 2594s 324ms/step - loss: 0.0768 - acc: 0.9717 - val_loss: 1.5787 - val_acc: 0.7422\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 2554s 319ms/step - loss: 0.0657 - acc: 0.9766 - val_loss: 1.5699 - val_acc: 0.7565\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 2497s 312ms/step - loss: 0.0540 - acc: 0.9808 - val_loss: 1.8222 - val_acc: 0.7510\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 2314s 289ms/step - loss: 0.0495 - acc: 0.9829 - val_loss: 1.8442 - val_acc: 0.7531\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 2477s 310ms/step - loss: 0.0425 - acc: 0.9857 - val_loss: 1.9398 - val_acc: 0.7453\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 2535s 317ms/step - loss: 0.0384 - acc: 0.9871 - val_loss: 1.9599 - val_acc: 0.7406\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 2361s 295ms/step - loss: 0.0354 - acc: 0.9882 - val_loss: 2.0843 - val_acc: 0.7548\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 2540s 317ms/step - loss: 0.0316 - acc: 0.9895 - val_loss: 2.2278 - val_acc: 0.7360\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 15325s 2s/step - loss: 0.0308 - acc: 0.9900 - val_loss: 2.1134 - val_acc: 0.7457\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 2498s 312ms/step - loss: 0.0268 - acc: 0.9912 - val_loss: 2.0058 - val_acc: 0.7551\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 2585s 323ms/step - loss: 0.0251 - acc: 0.9918 - val_loss: 2.0359 - val_acc: 0.7574\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 2616s 327ms/step - loss: 0.0239 - acc: 0.9923 - val_loss: 2.0713 - val_acc: 0.7577\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 2471s 309ms/step - loss: 0.0222 - acc: 0.9928 - val_loss: 2.2025 - val_acc: 0.7462\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 2486s 311ms/step - loss: 0.0216 - acc: 0.9932 - val_loss: 2.1831 - val_acc: 0.7520\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 2459s 307ms/step - loss: 0.0200 - acc: 0.9938 - val_loss: 2.3750 - val_acc: 0.7405\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 2449s 306ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 2.2179 - val_acc: 0.7481\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 2423s 303ms/step - loss: 0.0179 - acc: 0.9944 - val_loss: 2.4397 - val_acc: 0.7424\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 2423s 303ms/step - loss: 0.0162 - acc: 0.9949 - val_loss: 2.4239 - val_acc: 0.7367\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 2427s 303ms/step - loss: 0.0154 - acc: 0.9952 - val_loss: 2.3216 - val_acc: 0.7481\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 2994s 374ms/step - loss: 0.0155 - acc: 0.9953 - val_loss: 2.2931 - val_acc: 0.7519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f991bda89b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the CNN model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                               target_size=(64, 64),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000,\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is for one convolution layer\n",
    "and below we try to improve the accuracy on the test set by adding another convolution layer.\n",
    "\n",
    "Along with the convolution layer we add the Pooling layer too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
